# 3DSSD: Point-based 3D Single Stage Object Detector

Update: 2023/06/15

## â„¹ï¸ Info
- Paper: [arxiv.org](https://arxiv.org/abs/2002.10187)
  - Submission date: 2020/02/24
  - Authors: Zetong Yang, Yanan Sun, Shu Liu, Jiaya Jia
  - Conf.: CVPR 2020
- Implementation: [dvlab-research/3DSSD](https://github.com/dvlab-research/3DSSD)
  - framework: tensorflow
  - Official code: Yes
  - License: MIT license
- Keywords: CV, Point Cloud, Detection

## ğŸ–¥ï¸ Setup commands to run the implementation
### 1. Create a docker container
```bash
# Set this repository absolute path (ex: /home/user/obarads.github.io)
git clone https://github.com/Obarads/obarads.github.io.git
cd obarads.github.io
OGI_DIR_PATH=$PWD

# Create a base image with cuda 8.0, cudnn 6.0, and ubuntu 16.04
BASE_IMAGE=ogi_cuda:cuda8.0_cudnn6.0_ubuntu16.04
docker build . -t $BASE_IMAGE  -f $OGI_DIR_PATH/public/data/envs/cuda/cuda8.0_cudnn6.0_ubuntu16.04/Dockerfile 

# Create and move to a container dir
mkdir containers
cd containers
# Clone the repository
git clone https://github.com/dvlab-research/3DSSD.git
# Move to 3DSSD
cd 3DSSD
# Switch to 2020/04/09 ver.
git switch -d 8bc7605d4d3a6ec9051e7689e96a23bdac4c4cd9
# Copy a folder for building env.
cp -r $OGI_DIR_PATH/environments/3P3SSOD/ ./dev_env

# Create docker image and container
docker build . -t 3dssd -f ./dev_env/Dockerfile --build-arg UID=$(id -u) --build-arg GID=$(id -g) --build-arg BASE_IMAGE=$BASE_IMAGE
docker run -dit --name 3dssd --gpus all -v $PWD:/workspace 3dssd
```

### 2. Setup packages
In a docker container:
```bash
cd /workspace

conda create -n 3dssd python=3.6
conda activate 3dssd

TENSORFLOW_PATH=~/anaconda3/envs/3dssd/lib/python3.6/site-packages/tensorflow
CUDA_PATH=/usr/local/cuda

cd dev_env
git apply code.diff
pip install -r requirements.txt

cd ../
bash compile_all.sh $TENSORFLOW_PATH $CUDA_PATH

export PYTHONPATH=$PYTHONPATH:/workspace/lib:/workspace/mayavi
echo 'export PYTHONPATH=$PYTHONPATH:/workspace/lib:/workspace/mayavi' >> ~/.bashrc
```

### 3. Setup the dataset
Please refer to [the section of README.md](https://github.com/dvlab-research/3DSSD/tree/8bc7605d4d3a6ec9051e7689e96a23bdac4c4cd9#data-preparation) for the [KITTI dataset](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d) preparation (`calib`, `image_2`, `label_2`, `velodyne` dirs) into the docker container, and then run the following commands:
```bash
mkdir -p dataset/KITTI/object
cd dataset/KITTI/object
wget https://github.com/dvlab-research/3DSSD/files/4491173/train.txt --no-check-certificate
wget https://github.com/dvlab-research/3DSSD/files/4491174/val.txt --no-check-certificate
wget https://github.com/dvlab-research/3DSSD/files/4491574/test.txt --no-check-certificate

gdown https://drive.google.com/u/0/uc?id=1d5mq0RXRnvHPVeKx6Q612z0YRO1t2wAp
unzip train_planes.zip -d training/

python lib/core/data_preprocessor.py --cfg configs/kitti/3dssd/3dssd.yaml --split training --img_list train
python lib/core/data_preprocessor.py --cfg configs/kitti/3dssd/3dssd.yaml --split training --img_list val
```

### 4. Run the model
In a docker container:
```bash
cd /workspace
python lib/core/trainer.py --cfg configs/kitti/3dssd/3dssd.yaml
```
## ğŸ“ Clipping and note
### ã©ã‚“ãªã‚‚ã®?
- Single stage 3Dç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã€3DSSDã‚’ææ¡ˆã—ãŸã€‚
- ç²¾åº¦ã¨åŠ¹ç‡ã‚’èª¿å¾‹ã•ã›ãŸè»½é‡ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚
- åŠ¹ç‡ã‚’ä¸Šã’ã‚‹ãŸã‚ã€æ—¢å­˜æ‰‹æ³•ã§å¤§æŠµæ¡ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å±¤ã¨refinement stageã‚’ã™ã¹ã¦çœãã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…å‰Šæ¸›ã—ãŸã€‚
- å°‘ãªã„ä»£è¡¨ç‚¹ã§ã®æ¤œå‡ºã‚’å¯èƒ½ã«ã™ã‚‹ãŸã‚ã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‡¦ç†ã«ãŠã‘ã‚‹fusion sampling strategyã‚’ææ¡ˆã™ã‚‹ã€‚
- ã¾ãŸã€å‡¦ç†é€Ÿåº¦ã¯38msã»ã©ã§ã‚ã‚ŠãªãŒã‚‰ã€æœ€æ–°ã®two stageæ‰‹æ³•ã»ã©ã®ç²¾åº¦ã‚’æŒã¤ã€‚
- å…¨ä½“åƒã¯å›³1ã®é€šã‚Šã€‚

![fig1](img/3P3SSOD/fig1.png)

> Figure 1. Illustration of the 3DSSD framework. On the whole, it is composed of backbone and box prediction network including a candidate generation layer and an anchor-free prediction head. (a) Backbone network. It takes the raw point cloud (x, y, z, r) as input, and generates global features for all representative points through several SA layers with fusion sampling (FS) strategy. (b) Candidate generation layer (CG). It downsamples, shifts and extracts features for representative points after SA layers. (c) Anchor-free prediction head.

### ã©ã†ã‚„ã£ã¦æœ‰åŠ¹ã ã¨æ¤œè¨¼ã—ãŸ?
ä»–ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ãŸéš›ã®Resultã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
- KITTIã«ã‚ˆã‚‹æ¤œå‡ºæ¤œè¨¼ (table 3)
- nuScenesã«ã‚ˆã‚‹æ¤œå‡ºæ¤œè¨¼ (table 4)

![tab3](img/3P3SSOD/tab3.png)

> Table 3. Results on KITTI test set on class Car drawn from official Benchmark [1]. â€œSens.â€ means sensors used by the method. â€œLâ€ and â€œRâ€ represent using LiDAR and RGB images respectively

![tab4](img/3P3SSOD/tab4.png)

> Table 4. AP on nuScenes dataset. The results of SECOND come from its official implementation [2].

## ğŸ“š References
ãªã—

