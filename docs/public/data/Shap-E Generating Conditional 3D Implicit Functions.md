# Shap-E: Generating Conditional 3D Implicit Functions

Update: 2023/07/09

## â„¹ï¸ Info
- Paper: [arxiv.org](https://arxiv.org/abs/2305.02463)
  - Submission date: 2023/05/03
  - Authors: Heewoo Jun, Alex Nichol
  - Conf.: None?
- Implementation: [openai/shap-e](https://github.com/openai/shap-e)
  - framework: Pytorch
  - Official code: Yes
  - License: MIT license
- Keywords: CV, Mesh, Object Generation

## ðŸ–¥ï¸ Setup commands to run the implementation
### 1. Create a docker container
```bash
# Set this repository absolute path (ex: /home/user/obarads.github.io)
git clone https://github.com/Obarads/obarads.github.io.git
cd obarads.github.io
OGI_DIR_PATH=$PWD

# Get a base image
BASE_IMAGE=nvidia/cuda:11.7.1-cudnn8-devel-ubuntu20.04
docker pull $BASE_IMAGE

# Create and move to a container dir
mkdir containers
cd containers
# Clone the repository
git clone https://github.com/openai/shap-e.git
# Move to shape-e
cd shap-e
# Switch to 2023/07/07 ver.
git switch -d db3f3b4c2f572b53fbe408d202e2445a1f656353
# Copy a folder for building env.
cp -r $OGI_DIR_PATH/environments/SGC3IF/ ./dev_env

# Create docker image and container
docker build . -t shape -f ./dev_env/Dockerfile --build-arg UID=$(id -u) --build-arg GID=$(id -g) --build-arg BASE_IMAGE=$BASE_IMAGE
docker run -dit --name shape --gpus all -v $PWD:/workspace shape
```

### 2. Setup packages
In a docker container:
```bash
cd /workspace

conda create -n shape python=3.9
conda activate shape

pip install -r dev_env/requirements.txt
pip install -e .
```

### 3. Run the model
Please open `/workspace/shap_e/examples/sample_text_to_3d.ipynb`, in the container and run the cells in `sample_text_to_3d.ipynb`.

## ðŸ“ Clipping and note
### ã©ã‚“ãªã‚‚ã®?
- > Unlike recent work on 3D generative models which produce a single output representation, ShapÂ·E directly generates the parameters of implicit functions that can be rendered as both textured meshes  and neural radiance fields.
- > We train ShapÂ·E in two stages: 
  - > first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function;
  - > second, we train a conditional diffusion model on outputs of the encoder.
- > Compared to PointÂ·E [41], a recently proposed explicit 3D generative model, our models converge faster and obtain comparable or superior results while sharing the same model architecture, datasets, and conditioning mechanisms.
- Fig.1 shows text-conditional meshes generated by ShapÂ·E.

![fig1](img/SGC3IF/fig1.png)
> Figure 1: Selected text-conditional meshes generated by ShapÂ·E. Each sample takes roughly 13 seconds to generate on a single NVIDIA V100 GPU, and does not require a separate text-to-image model.
ã€€
### ã©ã†ã‚„ã£ã¦æœ‰åŠ¹ã ã¨æ¤œè¨¼ã—ãŸ?
- ä»–ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ãŸéš›ã®Resultã¯ä»¥ä¸‹ã®é€šã‚Šã€‚
- > For most of our experiments, we employ the same dataset of underlying 3D assets as Nichol et al. [41], allowing for fairer comparisons with their method.
  - Note: the authors slightly extend the original post-processing (Section 4.1).
- Comparison to PointÂ·E (figure 4)
- Comparison to Other Methods (table2)

![fig4](img/SGC3IF/fig4.png)
> Figure  4: Evaluations  throughout  training  for  both  ShapÂ·E  and  PointÂ·E.  For  each  checkpoint  for  both  models,  we  take  the  maximum  value  when  sweeping  over  guidance  scales { 2.0, 3.0, 4.0, 5.0, 8.0, 10.0,15.0} .

![fig4](img/SGC3IF/tab2.png)
> Table 2: Comparison of 3D generation techniques on the CLIP R-Precision metric on COCO evaluation prompts. Compute estimates and other methodsâ€™ values are taken from Nichol et al. [41]. âˆ—The best text-conditional results are obtained using our expanded dataset of 3D assets.

### Other experiments
Encoder Evaluation, Limitations and Future Work

## ðŸ“š References
- [41] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen.  Point-e: A system for generating 3d point clouds from complex prompts. arXiv:2212.08751, 2022.
