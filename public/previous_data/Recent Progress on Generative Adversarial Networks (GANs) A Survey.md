# Recent Progress on Generative Adversarial Networks (GANs): A Survey

元の論文の公開ページ : [ieee](https://ieeexplore.ieee.org/document/8667290)
Github Issues : [#40](https://github.com/Obarads/obarads.github.io/issues/40)

## どんなもの?
GANのサーベイ。発行が2019/03/14(=つい最近)なのでありがたい。

## 先行研究と比べてどこがすごいの?
最新のGANのサーベイを提供してくれたこと。

## 技術や手法のキモはどこ? or 提案手法の詳細
以下全文翻訳、参考文献は本文と連動する。

### 要約
Generative adversarial network (GANs)はAI分野において最も重要な研究手法の一つであり、データ生成能力は注目を良く引く。この論文では、著者らがGANの最新の動向を紹介する。はじめに、GANの基礎理論と近年の生成モデルごとの違いについて分析、要約する。次に、GANの派生モデルをひとつずつ分類、紹介していく。3つめに、トレーニングの戦略と評価方法を紹介する。4つめに、GANのアプリケーションを紹介する。最後に、取り組むべき問題とその将来の方針について議論する。

### 1章 導入
過去数年間で、コンピュータサイエンスとデータ蓄積で大きな発展があった。AIは価値ある研究の話題と多数の有意義なアプリケーションとともに盛況な分野となった。AIのコミュニティでは、機械学習[1]が我々の日常の様々な場面で大きな影響を振るうようになった。機械学習アルゴリズムはすべて、与えられたデータの表現を必要とする。しかし、他の分野やタスク中でこれらの技術を使いたいと望んだとしても、有用な特徴を抽出することが難しいものである。そこで、研究者は分類や検出をするときに役に立つ情報を自動的に抽出する表現学習[2]と呼ばれる新しいアプローチを提案した。深層学習[3]はいくつかの簡単な表現を構成することによって、他の手法よりもより抽象的な特徴(=高レベルな特徴)を容易に抽出できる表現学習手法の一種である。 

一般的に、ラベルがあるかどうかで機械学習の手法は教師あり学習か教師なし学習の2つに分けられる。教師あり学習には異なる表現を含むデータセットが必要とされ、データセット中の各サンプルにはラベルがつけられている。教師あり学習の代表には分類、回帰、構造問題(structured output learning等)がある。しかしながら、教師なし学習はわずかなラベルを含むデータセットを必要とする。目的は、データセット内にある特有の構造を探し出すことである。通常、密度推定、クラスタリング、合成、ノイズ除去が教師なし学習とみなされる。

教師あり学習のために自動的にラベルを付けたり収集したりすることは困難である。したがって、研究者は教師なし学習により注目する。教師なし学習のタスクにおいて、生成モデルは最も有望な技術の一つである。典型的な生成モデルは基本的にマルコフ連鎖、最大尤度、近似的推論をベースとしている。制限付きボルツマンマシン[4]とその拡張モデル(Deep Belief Network[5]、Deep Boltzmann Machinescite[6])は常に最大尤度推定をベースとしている。これらの手法によって生成されたモデルは分布を生成し、そしてこれらの分布は訓練データの経験分布と一致することを目的とした多くのパラメーターを持つ。

しかしながら、これらの初期のモデル[4]-[6]は深刻な制限を持ち合わせており、もしかしたら望まれるような一般化ではないかもしれない。2014年に、Goodfellow氏がGANと呼ばれる新たな生成モデルを提案した。GANは、ゲーム理論をベースとした、生成器と弁別器の二つのネットワークから成り立っている。生成器の役割は弁別器をだますことができるほどリアルな画像を生成することである。弁別器の役割は、実際のデータと偽物のデータを識別することである。この場合、ドロップアウトアルゴリズムと逆伝播[8]を使って両方のモデルを訓練する。近似的推論かマルコフ連鎖はGANには必要とされない。

この調査では、ネットワークのアプリケーション、動機、定義を含む、最新のSOATなGANを要約&分析する。サーベイの作りは以下の様になっている。2章はいくつかの生成モデルを紹介し、GANの基礎理論に注目する。さらに、これらのモデルの簡単な比較も提供する。一連の派生GANモデルは3章で紹介される。4章では、GANのいくつかの訓練の仕組みを紹介する。5章では様々な評価方法の良し悪しについて議論する。様々な分野でのGANのアプリケーションは6章で概観される。7章では、GANの制限と将来的な提案について検討する。最後に8章で結論付ける。

### 2章 生成モデルとGAN
GANは深層生成モデルの一つであり、生成問題をうまく処理することができる。この章では、はじめに一般的な深層生成モデルのいくつかの種類を紹介し、次にこれらのモデル間で違いを比較する。その次に、基本的なGANの理論とアーキテクチャについて紹介する。

#### A. 深層生成モデル
AIの狙いは人間界の様な複雑な世界を理解することである。このアイデアに基づいて、AIの研究者が統計と確率の観点からそれらの周りの世界を描くことに専念する生成モデルを提案した。いつもお世話になっております。今現在、生成モデルはGANs[7]、VAE[9]、AutoRegressive Networks[10]の3つのカテゴリに分けることができる。VAEは確率的なグラフィカルモデルであり、データの確率分布のモデル化すること目的としている。しかしながら、最終的な確率論的シミュレーションはいくらかのバイアスを持つ。そのため、GANsよりもぼやけたサンプルが多く生成される。PixelRNN[11]はautoregressive networksの一つであり、画像生成の問題をピクセルの生成と予測の問題に転換する。それゆえ、各ピクセルは一つ一つ処理される必要があるが、GANはワンショットでサンプルを直接処理する。そして、これはGANがPixelRNNよりも早いサンプル生成をする。

確率的生成モデルとして、確率密度が提供されないとき、データの自然な解釈に依存する従来の生成モデルのいくつかは、訓練及び適応ができない。しかし、GANは非常に利口な内部の敵対的訓練メカニズムを導入しているため、GANsはこのような状況でも依然として使うことができる。

#### B. GANsの原理
GANsはゲーム理論に触発されたものであり、生成器と識別器がお互い、訓練中にナッシュ均衡を達成しようとする。GANのアーキテクチャを図1に示す。生成器$G$の動作原理は実際のデータの潜在分布を極力適合させるために偽のデータを生成することである。一方で、識別器$D$の動作原理は偽か実際のデータか正しく見分けることである。生成器の入力はランダムノイズベクトル$z$(基本的には一様分布もしくは正規分布)である。ノイズは多次元ベクトルである偽のサンプル$G(z)$を得るために生成器$G$を介して新しいデータ空間にマッピングされる。また、識別器$D$は二値分類器でありデータセットからの実際のサンプルもしくは生成器$G$から生成された偽のサンプルを入力として受け取る。そして、識別器$D$の出力は実際のデータである確率である。識別器$D$が実際のものか偽のものかどうかわからなくなった時、GANは最適な状態になる。この時点で、実際のデータ分布を学習した生成器モデル$G$が得られる。

![fig1](img/RPoGANAS/fig1.png)

#### C. GANsの学習モデル
このゲーム理論中の識別機と生成器は対応する自身の損失関数を持つ。このとき、これらをそれぞれ$J^{(G)}$と$J^{(D)}$と呼ぶ。[7]中では、識別器$D$が二値分類器として定義され、損失関数はクロスエントロピーで示される。定義は式(1)の通り。

![eq1](img/RPoGANAS/eq1.png)

ここで、$x$は実際のサンプルを示し、$z$は$G(z)$を生成器$G$で生成するためのランダムノイズベクトル、$\mathbb{E}$は期待(期待値、expectation)である。$D(x)$は$D$が$x$を実際のデータとみなす確率、$D(G(z))$は$D$が$G$によって生成されたデータを特定する確率を示す。$D$の目的はデータの出所を正しく突き止めることであるため、$D(G(z))$が0に近づくことを目標とするが、$G$は1に近づくことを目的とする。この考えに基づいて、２つのモデル間には対立が存在する(ゼロサムゲーム)。したがって、生成器の損失は識別機によって式(2)の様に導出される。

![eq2](img/RPoGANAS/eq2.png)

結果的に、GANsの最適化問題はminimaxゲームに変換される。定義は式(3)の通り。

![eq3](img/RPoGANAS/eq3.png)

訓練プロセス中に$G$中のパラメーターは$D$の更新プロセスのパラメーターと一緒に更新される。$D(G(z))=0.5$である時、識別機はこれらの2つの分布間の差異を特定することができなくなる。この状態では、モデルが大域的最適解を達成するだろう。

### 3章 派生GANsモデル
オリジナルのGANsの欠陥により、様々な派生GANsモデルが提案され、これらの派生GANsモデルはアーキテクチャ最適化ベースのGANsと目的関数最適化ベースのGANsの2種類のグループに分けられる(表1)。このセクションでは、いくつかの派生モデルの詳細について紹介する。

![tab1](img/RPoGANAS/tab1.png)

#### A. アーキテクチャ最適化ベースのGANs
##### 1. 畳み込みベースのGANs  
CNN[30]は教師あり学習のとても効率的なモデルとして見なされており、画像処理分野では最も普及しているネットワークの構造の一つである。生成器と識別機のネットワーク構造に関しては、オリジナルのGANsがMLPを採用している。画像の特徴抽出に関してはMLPよりもCNNの方が優れているため、Radfoldら[12]はDeep Convolutional Generative Adversarial Networks(DCGAN)を提案した。図2に示す通り、このアプローチは生成器内の全結合層をdeconvolution層に置き換える革新的なものであり、画像生成タスクにおいて素晴らしい結果を残した。

![fig2](img/RPoGANAS/fig2.png)

##### 2. 条件付きベースのGANs  
生成器の入力はランダムノイズベクトル$z$であるため、これらの制限されていない入力が訓練モードの崩壊を起こす可能性がある。それゆえ、MirzaとOsindero[13]はConditional Generative Adversarial Networks(CGANs)を提案した。CGANsは識別機と生成器の両方に条件変数$c$(変数$c$はラベルやテキストなどのデータ)を導入している。この変数$c$を導入することで、モデルに条件を与え、データ生成プロセスに影響を与える。図3(a)に示すように、生成器の入力は条件変数$c$とノイズベクトル$z$であり、識別機の入力は生成器からの出力$G(z|c)$と条件変数$c$から成り立つ実際のサンプルである。したがって目的関数は式(4)のように示される。

![eq4](img/RPoGANAS/eq4.png)

追加で、Chenら[14]はInfoGANと名付けられた他のCGANsを提案した。相互情報量を導入することで、InfoGANは生成プロセスをより制御しやすくし、結果をもっと解釈しやすくなった。ここで、相互情報量は生成されたデータ$x$と潜在コード$c$間の矯正を表す。$x$と$c$の関係性を向上させるため、相互情報量の値は最大化される必要がある。その生成精機はCGANsに似ているが、潜在コード$c$がわからないという違いがあるため、訓練プロセスを通して発見する必要がある。オリジナルのGANsの識別機に加えて、InfoGANは条件変数$Q(c|x)$を出力するため追加のネットワーク$Q$を持つ。目的関数は式(5)のようになる。

![eq5](img/RPoGANAS/eq5.png)

ここで、$\lambda$は制約関数$I(c,G(z,c))$のハイパーパラメータであり、相互情報量によって、生成されたデータに対して潜在コード$c$がより一層合理的になる。InfoGANのアーキテクチャを図3(b)に示す。  
CGANsをベースとした、Odenaら[15]はAuxiliary Classifier GAN(ACGAN)を提案した。図3(c)では、識別機に対して条件変数$c$は追加されず、代わりに他の分類器でクラスラベルの確率を示すために使われる。次に、損失関数は正しいクラスラベル予測確率を増やすために修正される。

![fig3](img/RPoGANAS/fig3.png)

##### 3. オートエンコーダーベースのGANs  
オートエンコーダーは入力と同じものを再構築するように訓練するニューラルネットワークの一種である。エンコーダー$z=f(x)$とデコーダー$\hat{x}=g(z)$の二つのパーツで成り立っており、ここでエンコーダーは入力$x$(画像、ビデオ、音響、テキストデータなど)を隠れ層(潜在コード$z$)へ変換するために使わるものであり、次元を削減するプロセスである。デコーダーは入力として隠れ層$h$からコードを受け取るために使われる。訓練後、デコーダーは入力$x$と同じものを出力$\hat{x}$として再構築しようとする。入力との差を損失とするため、ラベルを必要しない教師なしモデルである。近年では、潜在可変モデル理論と組み合わせてオートエンコーダーを生成モデルに適応するために使用されている。  
また、オートエンコーダーはエンコーダによって得られる隠れ層が空間中で均一に分布しないため不完全であり、結果として分布内で多数のギャップを生む。従って、Makhzaniら[16]はオートエンコーダーと敵対的ネットワークのアイデアを組み合わせたAdversarial Autoencoder(AAE)を提案した。このアプローチは、任意の事前分布がエンコーダーによって得られた隠れ層の分布に課される。その任意の一部分から意味を持つサンプルをデコーダーが再構築できるように、事前分布中のギャップをなくすことを保証する。AAEのアーキテクチャは図4に示すとおりであり、潜在空間$z$(隠れ層)が偽物のデータを表現し、$z'$が特定の分布$p(z)$による事前分布を表す。これらは識別器の入力となる。トレーニング後は、エンコーダーが望んだ分布を学習できるようになっており、デコーダーは最終的に、必要とされる分布によって再構成されたサンプルを出力できるようになる。

![fig4](img/RPoGANAS/fig4.png)

いくつかのモデル[17]-[19]はGANsにエンコーダーのみを付け加えた。これらのモデルの生成器は潜在空間中の特徴を学習し、そしてデータ分布の意味的変化を捉える。しかし、データサンプルの分布から潜在空間にマッピングすることができない。この問題に対処するため、Donahueら[17]は有効な推論をするだけでなく生成されたサンプルの質を保証もすることができるBidirectional Generative Adversarial Networks(BiGAN)を提案した。BiGANとALIのアーキテクチャを図5に示す。BiGANのアーキテクチャでは、識別器と生成器に加えて、潜在特徴空間にエンコーダーがそのモデルに追加される。このエンコーダーはデータ分布においてGANsによって生成されたデータを潜在特徴空間に逆マッピングするために使われる。この識別器の入力はデータで構成されたタプルとそれに対応する潜在コードになる。生成器によって生成されたデータのため、このタプルは生成されたデータ$G(z)$とデータを生成するために使われるノイズベクトル$z$である。データセットからの本物のサンプル$x$のために、こちらのタプルはサンプル$x$とエンコーダーの逆マッピングを介して$x$から得られた$E(x)$である。このアプローチでは、エンコーダーが識別器のための特徴取得手段として使われる。同様に、Dumoulinら[18]は潜在特徴分布を学習するためのエンコーダを使うAdversarially Learned Inference(ALI)を提案した。この２つのアプローチはどちらも、並列で生成器とエンコーダーを学習することができる。  
先程説明した敵対的ネットワークとオートエンコーダーを組み合わせたアプローチに加えて、Ulyanovら[19]は敵対的ネットワークが生成器とエンコーダー間で動作し、ネットワークが識別器の導入を必要としないAdversarial Generator-Encoder Network(AGE)と名付けられたアプローチを提案した。図5にRが再構成損失関数を表すAGEのアーキテクチャを示す。モデルの構造において、生成器の狙いは潜在分布$z$と生成されたデータ分布間のダイバージェンスを最小化することであり、一方でエンコーダーの狙いは$z$ろ$E(G(z))$間のダイバージェンスを最大化することと、本物のデータ$x$のダイバージェンスを最小化することである。更に、これらはモード崩壊にならないように避けるための再構築関数を導入している。

![fig5](img/RPoGANAS/fig5.png)

2章において、著者らはGANsを用いてVAEsと簡潔な比較をした。VAEsの利点はモード崩壊による影響をより少なくすることであるが、生成されたサンプルがぼやけている。GANsに基づいて生成されたモデルはVAEsのサンプルより高品質なものを生成するが、モード崩壊の問題を持ち合わせている。Larsenら[20]はVAEsのでコーターをGANsの生成器に置き換え、VAEsとGANsの利点を組み合わせた。このアプローチで、GANsの敵対的損失をVAEsの目的関数と組み合わせ、ぼやけた画像を生成するVAEsの問題を減らし、一方で潜在コードの分布を学習できるVAEsの特性をキープした。

#### B. GANsベースの目的関数最適化
GANsの安定性を高めるために、目的関数を最適化することによって多くの試行錯誤[21]-[29]がなされた。GANsの訓練プロセスを調節するため、生成器を向上させるための勾配ベースの損失関数を使う。そして、オリジナルのGANsは生成器の損失関数を最小化するためのJensen-Shannon(JS)ダイバージェンスを最小化することによって目的を達成した。参考文献[22]はこれが特別なケースであるとし、任意のf-ダイバージェンスがGANsのアーキテクチャで使用できるということを指摘した。参考文献[22],[24]と[26]はGANsの安定性を向上させるための目的関数を構築するために異なるダイバージェンスを使った。  
GANsの安定性を改良するための他の手法は異なる正則化を使うことである。Cheら[23]は学習をより安定するための2つの正則化を提案した。もし、生成されたデータの分布と実際のデータの分布間が重なっていないもしくは無視しても問題ない程度に重なっている場合、ダイバージェンスは定数としてセットされる。このとき、勾配はゼロになり、勾配消失問題を引き起こす可能性がある。この問題を解決するために、Arjovskyら[27]はWasserstein Generative Adversarial Networks(WGAN)を提案する。これらは理論的にEarth-Mover(EM)距離が他の距離計量と比べて分布学習の勾配挙動をより良くすることを示した。このアプローチはLipschitz制約を強制するために重み付けされたクリッピング手法を提供し、そして不安定な訓練プロセスの問題に対処するための新しい損失計量を見つけた。Gulrajaniら[28]はWGANが識別器で重みのクリッピングの使用が原因で収束できないもしくはまだ残念な結果になるかもしないことを見つけた。従って、彼らはLipschitz制約を強制するためのWGAN-GPと名付けられた勾配罰則を提案した。手法はオリジナルのWGANよりも良いパフォーマンスを残し、ハイパーパラメータのチューニングをほとんどせずに既存のものよりも安定した多様なGANsアーキテクチャのトレーニングを可能にした。更に、Petzkaら[29]はWGAN-LPとして知られるLipschitz制約を強制するための新しい罰則化項を提案した。この手法はネットワークのトレーニングにさらなる安定性をもたらした。  
追加で、オリジナルのGANsは識別器が本物のサンプル分布でなんの制約なしに無限にモデル化する能力を持つことを仮定する。これは簡単に過剰適合と乏しい一般化能力を導く。GANsの能力を無限にモデル化することを制限するために、Qiら[25]はLipschitz連続関数を満たす空間への目的関数を最小化することによって得られる損失関数を制限した。[25]と[27]の両方は勾配消失とモード崩壊の問題に対処するためLipschitzの規則化を使った。違いは[27]のLipschitz制約がKantorovich-Rubinstein双対性から来ていることである。

### 4章 GANsの訓練の仕組み
GANsの目標はナッシュ均衡を達成することであるが、実行プロセスでは非常に難しい。この章では優秀な訓練パフォーマンスを達成するためのいくつかの提案を提供する。  
[31]では、Saliansらは訓練プロセスの安定化とパフォーマンスの改善をするためのいくつかの仕組みを提案した。はじめに、特徴マッチングは生成器に新しい目的関数を与えることによってGANsの訓練をより安定させることができる。この方法では、生成されたデータがより一貫性を持ち、そして生成器がよりサンプルの情報を生成するようにになる。第二に、ミニバッチ層を使うことで識別器がモード崩壊の問題を回避するためにサンプルの多様性を反映することを可能にする。第三に、historical averagingはモデルが収束することを助ける。パラメーターの平均値と現在値に大きな差異がある場合、生成器と識別器に現在のパラメーターに対する罰則を課すための項が追加される。第四に、one-sidedラベル平滑化は本物のサンプルのために識別器の推定を1に隣接する値にセットするために提案され、これは分類器の境界を平滑化することができる。  
別々の学習率を使うことによって、[32]はモデルが安定した局所ナッシュ均衡に収束できるよう保証するために、識別器と生成器のためのtwo time-scale update rule(TTUR)を提案した。[33]では、Miyatoらがスペクトル正規化を提案した。これはGANsの識別器の訓練を安定化させる重み正規化テクニックである。これらのアプローチは識別器で制約としてLipschitz定数を追加することである。[27]や[28]の勾配罰則ｔｐ重みのクリッピングとは違い、訓練を安定化させる目にスペクトルノルムを制限する。アプローチの計算コストは小さく、他のハイパーパラメーターを調節する必要がない。[34]では、Zhangらが生成器でスペクトル正規化を使うこともまた役に立つことを実証した。

### 5章 評価の計量手法
最近では、GANsモデルは異なるタスクに適応され、各タスクが自身の評価方法を持つ。しかしながら、まだ普遍的に定量的な評価手法が存在せず、研究者が異なるタスクのための評価指標をどのように決めるかについてかなりの混乱がある。それ故、著者らは今現在広く使われているいくつかの評価方法について紹介し、それらの長所と短所について議論する。

#### A. INCEPTION SCORES (IS)
この計測はGANsで広く使われており、Salimansら[31]によって提案された。高いISは生成されたモデルが高い品質を持つサンプルを生成できることを示し、サンプルは多様性を持つ。しかしながら、ISには限界があり、もし生成モデルがモード崩壊に陥った場合、ISが優れているように見えるかもしれないが実はとてもまずい状況になる。

#### B. MODE SCORE (MS)
ISがベースであり、Nowozinら[22]がMSというもう一つの評価測量を提案した。MSは生成されたサンプルの多様性と視覚的な質を同時に反映できる。この評価手法はground truthラベル上の事前分布に繊細でないISの問題に対処する。

#### C. FRÈCHET INCEPTION DISTANCE (FID)
FIDはHeuselら[32]によって提案された。FIDはクラス内のmode droppingを検知するために使われる。このアプローチでは、生成されたサンプルがInception networkの特定の層によって提供されている特徴空間に埋め込まれる。仮定に基づいて生成されたサンプルは多次元ガウスに従い、平均と分散が生成されたサンプルと本物のデータ間で計算される。次に、FrÈchet距離は２つのガウス間が生成されたサンプルの質を評価するために計測される。しかしながら、ISとFIDは過剰適合問題をうまく処理することができない。問題に対処するため、Kernel Inception距離(KID)がBinkowskiら[35]によって提案された。

#### D. MULTI-SCALE STRUCTURAL SIMILARITY FOR IMAGE QUALITY (MS-SSIM)
２つの画像間の類似性を測るために使われるsingle scale SSIM[36]測量とは異なり、最初にWangら[37]がマルチスケールな画像の品質評価のためのMS-SSIMを提案した。MS-SSIMは人の知覚的類似性の判断を予測することによって画像の類似性を定量的に評価できる。Odenaら[15]とFedusら[38]が生成されたデータの多様性を計測するためにこの評価方法を使った。参考文献[39]はIFDとISがサンプルの多様性をテストするためにMS-SSIMを用いて補助的な評価手法として使われている。  
更に、Classifier Two-sample Tests(C2ST)[40]は2値分類器の訓練に基づくもう一つの測量である。異なるサンプルが同じ分布から来たかどうか評価する。1-Nearest Neighbor classier (1-NN)[41]は多量のハイパーパラメーターチューニングと特別な訓練を必要としないC2STの確率変数である。  
目的関数はまた、モードがこれらの課題に向いているかどうか判断するための測量として使われる。Wasserstein Critic[27]とMaximum Mean Discrepancy (MMD)[42]は生成されたサンプル分布と本物のサンプル分布間の距離を測るために提案された。もしターゲットと出力間の分布が似ている場合、両方共低い値を持つ。  
どのように適切な評価指標を選ぶかは依然として難しい問題であり、[43]は研究者が定量的な評価指標を選ぶためにガイドするためのメタ指標としていくつかの指標を提示した。良い評価指標は本物のサンプルと生成されたサンプルを見分け、モード崩壊とmode dropを確かめ、過剰適合を検知するべきである。著者らはGANsモデルの質を評価するためのより適切な方法があることを願う。

### 6章 GANsのアプリケーション
生成モデルの一種として、GANsの最も直接的なアプリケーションはデータの生成である。それは本物のサンプルの分布から学習することで、分布と一致するサンプルを生成することである。この章は自然言語処理やコンピュータービジョン、その他の分野の厳選したGANsのアプリケーションを紹介する。

#### A. コンピュータービジョン
今のところ、成功したGANsのアプリケーションの殆どはコンピュータービジョン分野にある。例えば、画像変換、画像の超解像、画像合成とビデオ生成などである。以下にアプリケーションの詳細を紹介する。

##### 1) 画像の超解像
画像の解像度を改善するために、Super-Resolution Generative Adversarial Networks (SRGAN)がLedigら[44]によって提案された。これは低解像度画像を入力として受け取り、4xのアップサンプリングで高解像度画像を生成する。SRGANによって生成されたテクスチャのリアルさが不十分であるという問題に対処するため、また他の多くのノイズをともナウ問題があるため、Wangら[45]はEnhanced Super-Resolution Generative Adversarial Networks (ESRGAN)を提案した。ESRGANでは、ネットワーのアーキテクチャ、敵対的損失と知覚損失が改良された。更に、relativistic GAN [46]に基づいたResidual-in-Residual Dense Block (RRDB)と名付けられた新しいネットワークユニットが導入された。生成パフォーマンス比較を図6に示す。著者らはESRGANがSRGANよりも良い結果を残したことを確認した。

![fig6](img/RPoGANAS/fig6.png)

##### 2) 画像変換
画像の内容をあるドメインから他のドメインに変換するimage-to-image変換アプローチはIsolaら[47]が使うpix2pixと名付けられたCGANsによて提案された。実験はpix2pixがビジョンタスクのみならずグラフィックタスクにも効果があることを示した。この研究に対して、更に生成されたサンプルの質と精細度が向上したpix2pixHD[48]が生み出された。このアプローチは$2048\times 1024$の解像度画像を生成するための新規の敵対的損失項を使っている。pix2pixは画像変換問題のために使われるが、XとYの空間がペアになった訓練の空間を必要とする。しかしながら、我々の日常生活では、そのようなペアとなったデータを見つけるのは難しい。そのような場合を想定したものとして、CycleGAN[49]、DiscoGAN[50]、DualGAN[51]がサイクルの整合性のアイデアを適応した、X空間とY空間のデータがペアとなっているデータがなくても使えるモデルが提案されている。これらの3つの生成器は全てエンコーダー-デコーダーのフレームワークである。違いはエンコーダーとデコーダーで異なる特徴表現が使われることである。図7では、CycleGANのパフォーマンスを示している。これらの研究は２つのドメインをimage-to-image変換するものである。Choiら[52]はStarGANを提案した。StarGANは一つのモデルで複数のドメイン動詞で画像変換問題を解くことを可能としている。特に、表情合成と表情特性伝達のタスクにおけるStarGANの使用は驚異的な効果を持つ。

![fig7](img/RPoGANAS/fig7.png)

##### 3) テクスチャ合成
テクスチャ合成は画像分野において非常に典型的な問題である。GANsベースで、LiとWand[53]はテクスチャ合成アプローチを提案した。これはMarkovian Generative Adversarial Networks(MGAN)と名付けられている。Markovianパッチのテクスチャデータをキャプチャすることによって、リアルタイムテクスチャ合成を実現するため、MGANは非常に短い時間で定型化されたビデオと画像を生成することができる。Jetchevら[54]はSpatial GAN(SGAN)を提案した。これはテクスチャ合成で完全な教師なし学習を用いた初めてのGANsアプローチである。SGANに続いて、Bergmannら[55]はPeriodic Spatial GAN(PSGAN)を提案した。これは複雑な巨大データセットもしくは単体画像からperiodic textures(?)を学習することができる。それに加えて、ノイズ空間のテクスチャ情報を扱い、高画質なテクスチャを合成できる。

##### 4) 顔合成
顔合成もまた重要な目標である。現実的な顔のサンプルをどのように生成するかは、人々が取り組む必要がある問題である。Huangら[56]はTwo-Pathway Generative Adversarial Network(TP-GAN)を提案した。これは、人間のような局所と大域情報の両方を考慮することができ、単体の画像から高解像度の正面顔画像を合成するために使用される。この手法によって合成された顔画像はそれ固有の特徴を保持でき、異なる姿勢や照明の多数の画像を処理できる。  
更に、画像合成のために、Zhangら[34]はself-attentionブロックをGANに組み込み、長期依存性(?)を扱えるようになった(SAGAN)。これは識別器が2つの遠い特徴間の依存性を特定できるようにした。このアプローチは更に画像合成の質を向上させた。SAGANをベースとして、Brockら[57]はBigGANを提案した。BigGANは「truncation trick」の使用とバッチサイズの増加によって生成されるサンプルの忠実度と多様性を上げた。潜在分布$z$のため、従来のアプローチは生成器$G$の初期層に入力として$z$を埋め込むものである。しかしBigGANでは、$z$は異なる解像度とレベルの特徴に影響を与える生成器$G$の多層に埋め込まれる。ImageNet上で、Inception Score(IS)が166.3に届き、一方でFrechet Inception Distance(FID)は9.6に落ちた。図8にその結果を示す。ビデオ生成では、Tulyakovら[58]が教師なし手法でビデオを生成するためのMoCoGANを提案した。text-to-image変換では、[59]と[60]が画像を生成するためにテキスト記述を使った。

![fig8](img/RPoGANAS/fig8.png)

#### B. 自然言語処理
いまでは、GANsが音声処理と言語分野で多くの実績を残している。Yuら[61]はSeqGANを提案した。これはpolicy gradientに基づいて生成器を訓練する。実験はSeqGANが音楽、詩、音声の表現で従来の手法を上回ることができることを示した。Linら[62]は文章を生成するためにRankGANを提案した。これらは識別器の代わりにrankerを使い、素晴らしい結果を出した。Liら[63]は敵対的訓練手法を使うことによってオープンドメインの対話を生成した。このタスクは識別器と生成器の合同訓練、強化学習問題[64]として扱われる。識別器を使った結果は生成器に報酬を与えるための強化学習の報酬部分として使われ、push generatorによって生成される対話が人間の会話に似るようになる(?)。

#### C. 他の分野
GANsは他のドメインでも使われている。医療では、Schleglら[65]が医療画像の異常検知のためのAnoGANを提案した、これは健康の特徴を含むデータセットを学習することで損傷箇所を探し出すというものである。Killoranら[66]は蛋白結合を最適化するためのGANを使用することでDNAシーケンスを生成した。セキュリティ分野では、HuとTan[67]がマルウェアを生成するためにGANsを使った。更に、個人向けの製品カスタマイズのために、Hwangら[68]は医療製品の製造にGANsを利用した。

### 7章 議論
#### A. GANsの問題点
GSNsの開発の主な問題として、モード崩壊が挙げられる。GANによって生成されたサンプルはいつも少数もしくはたった一つのモデルに集中する。これは生成されたサンプルが多様性を失うという結果につながる。それ故、生成されたサンプルの多様性をどうやって増やすか、依然としてその問題に取り組まなければならない。一つの解決策は評価の多様性を増やすためにサンプルのバッチを使うことである。4章のミニバッチ識別器がこのカテゴリの手法の一種である。更に、複数の生成器を多様なモデルを得るために使うことがもう一つの解決策である。参考文献[69]はモード崩壊に対処するための異なるモデルによって生成されたサンプルを組み込んでいる。更に、目的関数を最適化することで同様に問題に対処できる。WGAN[27]とunrolled GAN[21]は解決手法の代表作である。  
訓練仮定の不安定さもまた研究者が取り組まなければならない問題である。GANsは訓練中にナッシュ均衡に至る必要があるが、これは困難であることがわかっている[27]。3、4章で[27]-[29]、[31]-[33]はさらなる安定した訓練のために独自の解決方法を提案した。将来的には更に安定したGAN訓練をする&ナッシュ均衡に向かうためにより多くの解決策が提案されるべきである。  
他の生成モデルと比べて、GANの評価問題はより難しくなる。5章は現在広く使われているいくつかの評価手法とそれらの選択の仕方の提案を提供している。それ故、これは将来的に依然として対処される必要がある目標の一つである。

#### B. GANsの将来
ネットワークのアーキテクチャとアルゴリズムの改良による、人間にとって識別するのが難しいテキスト、ビデオ、オーディオと画像を生成するより強力な生成モデルの設計を望む。特に、テキスト分野でのGANsの使用は、情報検索(IR)と自然言語処理(NLP)において継続的な開発のための分野が依然として存在する。  
更に、近年GANsと強化学習(RL)の関係性もまた有望な研究の指標である。例えば[70]は、模倣学習にGANsを埋め込んでいる。[61]はpolicy gradientをGANsに組み込んでいる。[71]はactor-criticアルゴリズムとGANsの関係性を説明している。他にも色々ある。  
セキュリティの分野では、GANsが非常に役に立っている。ニューラルネットワーク上の敵対的攻撃は人が研究する必要のある人気の分野の一つである。もし入力のサンプルに少量の摂動がある場合、ニューラルネットワークが予測や分類を謝る可能性がある。現在は、CNN[72]-[74]、RNN[75]、Deep Reinforcement Learning(DeepRL)[76],[77]とGANs[78]への攻撃の研究がすでにある。いまでは、潜在性の敵対的攻撃に対する深層学習の脆弱性が一般的な事象である\[79\](?)。敵対的攻撃に対抗するため、[80]と[81]は正確に防御するためGANsを使った。将来的に、敵対的攻撃にさらなる堅牢性を持つように願う。更に、情報隠蔽(?)で、HayesとDanezis[82]が電子迷彩技術に敵対正技術を導入した。この技術は技術者がそのような問題に対処するため新しいアイデアを提供した。  
上記で提案された研究の方針に加えて、GANは教師なし学習として提案されたが、実際には、一定数のラベルを追加することが生成能力をかなり改良することにつながる。データラベルを大量に得ることは難しいが、少量のラベルを得ることはできる。それ故、GANと半教師有り学習をうまく組み合わせることもまた将来の一つの目標となる。

### 8章 結論
この論文ではGANsの研究背景を要約し、基礎原理を説明し、生成されたモデルと多様な分野のアプリケーションを紹介した。更に、評価手法と訓練手法もまた議論された。最後にGANsの既存の問題が要約され、将来的な研究方針に注目した。

### 謝礼
この研究のために使われたTitan XpはNvidia Corporationによって寄付されました。

## どうやって有効だと検証した?
なし

## 議論はある?
なし

## 次に読むべき論文は?
- なし

## 論文関連リンク
この資料中の参考文献は論文中のReferenceを参照すること。

## 会議
IEEE

## 著者
Zhaoqing Pan, Weijie Yu, Xiaokai Yi, Asifullah Khan, Feng Yuan, Yuhui Zheng.

## 投稿日付(yyyy/MM/dd)
2019/03/14

## コメント
なし

## key-words
GAN, Survey, CV, 完了

## status
完了

